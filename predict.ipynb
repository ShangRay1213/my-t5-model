{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce3f4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration, AutoTokenizer, AutoModelForSequenceClassification\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "# --- Data Preparation ---\n",
    "\n",
    "# load a different sample for the main run of my script.\n",
    "df = pd.read_csv(\"/Users/shangray/Desktop/amazon_sample_10000.csv\")\n",
    "\n",
    "# Grab the 'review_text' column and convert it to a list.\n",
    "input_reviews = df[\"review_text\"].tolist()\n",
    "# For a quick test, I'll just work with the first few.\n",
    "# input_reviews = input_reviews[0:4]\n",
    "\n",
    "# Just taking a quick look at the first few items to make sure it loaded correctly.\n",
    "print(input_reviews[:5])\n",
    "\n",
    "# --- Model & Function Setup ---\n",
    "# Setting up the T5 models I need. I'm using two stages here.\n",
    "# Stage 1: This model classifies sentences from a review.\n",
    "# Stage 2: This one summarizes the results from Stage 1.\n",
    "\n",
    "# I'll define the paths to my trained models.\n",
    "stage1_path = \"/Users/shangray/Desktop/sch_project/t5_stage1_trained\"\n",
    "stage2_path = \"/Users/shangray/Desktop/sch_project/t5_stage2_trained_local\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0181d59e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shangray/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/shangray/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Hooked from the beginning, page one, I could not put this book down. I identified with every thought, word, and feeling. I was amazed by the courage of the writer, and his strength in sticking to his gut feelings in a recovery world that tells one that one's own best thinking landed them in the place that they occupy! That he did this with courage, dignity, fear,guilt, great love and sick will gave me a feeling of great hope for him, and all human beings at a particularly negative time in human history! The depth and breadth of his honesty was a homecoming for me and many like me.\", 'This one gift I could give my grown daughter. I read this book over and over to her as a child. It was in wonderful condition!', \"It's hard to appreciate Danzig anymore since his voice is shot, this cd proved it, I miss the old days.................\", 'I heard about this book and thought, \"Wow. Should be great. Here\\'s a guy who endured a major tragedy (losing both parents within a month). He should have something interesting to say\". I was wrong. One would think that such hardship would bring some sort of enlightenment. Nope. He can\\'t even write. He tries too hard to be funny/clever/witty/bitter, and comes off as an amateur. You will be disappointed.', \"I thought this was one of Ongala's original recordings to remind me of 'good old days' growing up along the lake shores of western Kenya. It turns out this is Ongala renditioning some of his old popular songs in the English language. Comes out as artistically weak and unoriginal in my opinion. However, if you never knew Ongala's work of the 80s, or are not a diehard fan of authenticity you may actually find this CD enjoyable.\"]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:   0%|          | 0/10000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Processing Review 1 ---\n",
      "Input:\n",
      "Hooked from the beginning, page one, I could not put this book down. I identified with every thought, word, and feeling. I was amazed by the courage of the writer, and his strength in sticking to his gut feelings in a recovery world that tells one that one's own best thinking landed them in the place that they occupy! That he did this with courage, dignity, fear,guilt, great love and sick will gave me a feeling of great hope for him, and all human beings at a particularly negative time in human history! The depth and breadth of his honesty was a homecoming for me and many like me.\n",
      "\n",
      "Stage 1 Output:\n",
      "positive: Identified with every thought, word, and feeling; Courage and strength of the writer; Strength in sticking to gut feelings in a recovery world; Courage, dignity, fear, guilt, great love, and sick will; Honesty and depth of honesty; Homecoming for many negative experiences negative: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:   0%|          | 1/10000 [00:06<16:56:17,  6.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stage 2 Output:\n",
      "Identified with every thought, word, and feeling, courage and strength of the writer, strength in sticking to gut feelings, courage, dignity, fear, guilt, love, and illness, honesty and depth of honesty, homecoming for many negative experiences. Positive: None, no negative experiences.\n",
      "\n",
      "--- Processing Review 2 ---\n",
      "Input:\n",
      "This one gift I could give my grown daughter. I read this book over and over to her as a child. It was in wonderful condition!\n",
      "\n",
      "Stage 1 Output:\n",
      "positive: Great gift for a grown daughter; Reads the book over and over; In wonderful condition negative: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:   0%|          | 2/10000 [00:07<9:09:41,  3.30s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stage 2 Output:\n",
      "Pros: ideal gift for a child, repeat reading, excellent condition. Cons: None.\n",
      "\n",
      "--- Processing Review 3 ---\n",
      "Input:\n",
      "It's hard to appreciate Danzig anymore since his voice is shot, this cd proved it, I miss the old days.................\n",
      "\n",
      "Stage 1 Output:\n",
      "positive: Danzig's voice is shot; CD proves it; Memories of the old days negative: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:   0%|          | 3/10000 [00:08<6:36:55,  2.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stage 2 Output:\n",
      "Positive: shot voice, CD proves it, memories of the old days positive: None. Negative: None.\n",
      "\n",
      "--- Processing Review 4 ---\n",
      "Input:\n",
      "I heard about this book and thought, \"Wow. Should be great. Here's a guy who endured a major tragedy (losing both parents within a month). He should have something interesting to say\". I was wrong. One would think that such hardship would bring some sort of enlightenment. Nope. He can't even write. He tries too hard to be funny/clever/witty/bitter, and comes off as an amateur. You will be disappointed.\n",
      "\n",
      "Stage 1 Output:\n",
      "positive: The book is well-written and interesting. negative: The author is incapable of writing.; He tries too hard to be funny, clear, witty, and bitter.; The author comes off as an amateur.; The reviewer is disappointed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:   0%|          | 4/10000 [00:11<6:50:36,  2.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stage 2 Output:\n",
      "Positive: well-written and interesting book. Cons: incapacity to write, excessive effort to be funny and witty, amateur appearance, disappointment from reviewer.\n",
      "\n",
      "--- Processing Review 5 ---\n",
      "Input:\n",
      "I thought this was one of Ongala's original recordings to remind me of 'good old days' growing up along the lake shores of western Kenya. It turns out this is Ongala renditioning some of his old popular songs in the English language. Comes out as artistically weak and unoriginal in my opinion. However, if you never knew Ongala's work of the 80s, or are not a diehard fan of authenticity you may actually find this CD enjoyable.\n",
      "\n",
      "Stage 1 Output:\n",
      "positive: Ongala's original recordings remind of 'good old days' growing up in western Kenya; Ongala renditions old popular songs in English; Enjoyable for those who have never known Ongala's work of the 80s negative: Comes out as artistically weak and unoriginal\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:   0%|          | 5/10000 [00:13<6:48:31,  2.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stage 2 Output:\n",
      "original recordings, English renditions of popular songs, enjoyable for newcomers. Cons: lack of artistic originality.\n",
      "\n",
      "--- Processing Review 6 ---\n",
      "Input:\n",
      "I read the book in my English lesson at school but it took a long time for me to finish the novel. In my opinion the first 100 pages of the beach were interesting and easy to read. But after this part it becomes boring. The describtion of the several problems in the community are written to long but the different characters trades are described too superficial so it was very hard for me to understand the relations between the Beachers.\n",
      "\n",
      "Stage 1 Output:\n",
      "positive: The first 100 pages of the beach were interesting and easy to read; The character's trades are described too superficially; The novel is enjoyable and enjoyable negative: The novel takes a long time to finish; The first 100 pages of the beach are boring after this part; The description of several problems in the community is written to long; The relationships between the Beachers are very difficult to understand\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:   0%|          | 6/10000 [00:17<8:28:24,  3.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stage 2 Output:\n",
      ": engaging and easy to read first 100 pages, superficial character descriptions, enjoyable and unpleasant aspects. Cons: long novel finish time, boring first 100 pages, long description of community problems, difficult to understand relationships between Beachers and Beachers.\n",
      "\n",
      "--- Processing Review 7 ---\n",
      "Input:\n",
      "I have the model without auto reverse- C3125- and the tape speed is too slow, a common problem with portables. Aiwa is best for normal tape speeds.\n",
      "\n",
      "Stage 1 Output:\n",
      "positive: Aiwa is best for normal tape speeds negative: Model without auto reverse is too slow; Common problem with portables\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:   0%|          | 7/10000 [00:19<7:00:39,  2.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stage 2 Output:\n",
      "Positive: ideal for normal tape speeds. Cons: slow model speed, common problem with portables.\n",
      "\n",
      "--- Processing Review 8 ---\n",
      "Input:\n",
      "I sold the most chocalate for a school fundraiser and here was my reward,A cheap video game system with graphics worser then a Gamecube. I have a Wii and I'm satisfied with it,so don't waste your cash and buy a Wii!\n",
      "\n",
      "Stage 1 Output:\n",
      "positive: Wii is satisfied with the purchase; Recommendation to buy a Wii negative: Graphics are worse than a Gamecube; Purchased a cheap video game system\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:   0%|          | 8/10000 [00:21<6:51:02,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stage 2 Output:\n",
      "Pros: satisfied Wii purchase, recommendation to buy a Wii negative: inferior graphics, cheap video game system. Cons: inferior graphics compared to Gamecube, poor video game system quality.\n",
      "\n",
      "--- Processing Review 9 ---\n",
      "Input:\n",
      "The difference between comedies of the past and \"family\" movies of today is their ability to be good entertainment for everyone w/o being hokey. Bud and Lou are in fine form in this, one of their last films. Everyone will enjoy this wonderfully funny film. Look for the \"Take Your Pick\" bit, one of those classic Abbott and Costello routines.\n",
      "\n",
      "Stage 1 Output:\n",
      "positive: Comedies of the past are good entertainment for everyone without being hokey; Bud and Lou are in fine form in this one of their last films; Wonderfully funny film; Featuring classic Abbott and Costello routines; Overall enjoyable experience for everyone negative: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:   0%|          | 9/10000 [00:24<6:46:30,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stage 2 Output:\n",
      "Positive: enjoyable past comedies, excellent Bud and Lou performances, humorous film, classic Abbott and Costello routines, enjoyable overall experience. Cons: None.\n",
      "\n",
      "--- Processing Review 10 ---\n",
      "Input:\n",
      "In this work of Plato, the author greatly strives to define justice. He tries to put forth what would be a perfect society. The problem with Plato is that he left Jehovah, the true God, out of his philosophy. Jehovah is the center of everything, and without Him, there is no truth. Since Plato rejected belief in the true God, and whole, intact truth is only found in that true God, Plato's philosophy is understandably flawed. Of course, as THE REPUBLIC is a popularly recongnized classic, one may find it useful to read it, in order to understand the viewpoint of the masses who accept it, provided one compares it with the truth of God's word, the Bible.\n",
      "\n",
      "Stage 1 Output:\n",
      "positive: Plato strives to define justice; He tries to put forth a perfect society negative: Plato left Jehovah, the true God, out of his philosophy; Plato rejected belief in the true God; Whole, intact truth only found in the true God; Plato's philosophy is understandably flawed; The Republic is a popularly recongnized classic; One may find it useful to read the book to understand the viewpoint of the masses who accept it\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:   0%|          | 10/10000 [00:29<9:03:38,  3.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stage 2 Output:\n",
      "philosophy, ideal society. Negative: justice-oriented philosophy, ideal society. Positive: rejection of Jehovah, rejection of belief in God, incomplete truth, understandably flawed philosophy, popularly recongnized classic, useful to understand the viewpoint of the masses.\n",
      "\n",
      "--- Processing Review 11 ---\n",
      "Input:\n",
      "a great simple easy to use, even for adults the basic regular everyday recipes from williams sanoma so you know its awesome. a great gift idea, especialy for new parents or kids special day. enjoy... worth every cent.\n",
      "\n",
      "Stage 1 Output:\n",
      "positive: Great simple easy to use recipe; Basic regular everyday recipes from Williams Sanoma are awesome; Great gift idea; Recommended for new parents or kids special day; Worth every cent negative: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:   0%|          | 11/10000 [00:31<8:00:55,  2.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stage 2 Output:\n",
      "Pros: simple and effective recipe, high-quality regular recipes, gift idea, suitable for new parents or kids, value for money. Cons: None.\n",
      "\n",
      "--- Processing Review 12 ---\n",
      "Input:\n",
      "My girls love to play with their play kitchen and to cook and bake! Santa brought this set of cupcakes and they are great. They are shape sorters, and yummy cupcakes! They fit perfect into their play oven. These are also made from quality materials.\n",
      "\n",
      "Stage 1 Output:\n",
      "positive: Great cupcakes; Shape sorters; Delicious cupcakes; Perfect fit into play oven; Made from quality materials negative: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:   0%|          | 12/10000 [00:32<6:50:43,  2.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stage 2 Output:\n",
      "Positive: quality cupcakes, shape sorters, tasty cupcakes, ideal for play oven, high-quality materials. Cons: None.\n",
      "\n",
      "--- Processing Review 13 ---\n",
      "Input:\n",
      "This book was actually really good. I read this book because in English we'd been focusing on different authors and Ann Rinaldi was one of them. I liked her writing style after seeing bits and pieces of it and fell in love with her writing. I read In My Father's House and once I completed that I just felt I had to read another of her works. The story never grows dull, which most usually do in English.I just love books that have girls my age from the South! I'm a big historic fiction reader and just love when I can relate to a character even in the smallest way. It was just a great book overall. There's nothing really more to say than it's just a really great book! Also if you liked this book- you'll love In My Father's House\n",
      "\n",
      "Stage 1 Output:\n",
      "positive: Ann Rinaldi's writing style was liked and loved; In My Father's House was enjoyable and enjoyable; The story never grows dull, which is typical in English; Suitable for girls from the South; Ability to relate to a character even in the smallest way; Overall great book overall; Recommended for those who liked the book negative: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:   0%|          | 13/10000 [00:36<7:29:07,  2.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stage 2 Output:\n",
      ": liked writing style, enjoyable In My Father's House, consistent story quality, suitable for girls, character-related character interaction, overall positive experience, recommended for those who liked the book. Cons: None.\n",
      "\n",
      "--- Processing Review 14 ---\n",
      "Input:\n",
      "the basic plot of the story is about o boy called kit and he playes a game called death whith his firends he gets in lots of trouble but from there other things come into the story like jhon askew [the ring leader of the game death] and also his grandpa get very ill and diesin mid januery the story was set in stonygate [in the wilderness] my favrout charicter is kit and his grand pa because they are realyclose to each otherand the worst is jhon askew becase he is bossy, evel and not what you call a firend.\n",
      "\n",
      "Stage 1 Output:\n",
      "positive: The story is set in stonygate in the wilderness.; The main character is kit.; The story is based on a game called death.; The main character is kit.; The story is based on the character's relationship with his grandpa.; The character is a favorite character. negative: The story is set in stonygate, in the wilderness.; The ring leader of the game death, jhon askew, and grandpa are both very close to each other.; The worst character is jhon askew, who is bossy, evel, and not a firend.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:   0%|          | 14/10000 [00:43<11:31:27,  4.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stage 2 Output:\n",
      "character's relationship with grandpa. Positive: wilderness setting, main character kit., based on death game, main character kit., based on grandpa's relationship with death. Positive: wilderness setting, close ring leader relationships, worst character jhon askew.\n",
      "\n",
      "--- Processing Review 15 ---\n",
      "Input:\n",
      "This is a collection with several good songs and a few very good ones.Mercy Me has solid Christian lyrics combined with a very pleasant light rock sound. Three songs that really stand out on this CD are the following: 'Here With Me', 'In The Blink of An Eye', and 'Homesick'.It is pretty good for all ages.\n",
      "\n",
      "Stage 1 Output:\n",
      "positive: Solid Christian lyrics; Pleasant light rock sound; Three songs stand out on the CD: 'Here With Me', 'In The Blink of An Eye', and 'Homesick'; Good for all ages negative: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:   0%|          | 15/10000 [00:45<9:45:46,  3.52s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stage 2 Output:\n",
      "Positive: Christian lyrics, light rock sound, distinctive songs, suitable for all ages. Cons: None.\n",
      "\n",
      "--- Processing Review 16 ---\n",
      "Input:\n",
      "Needed battery's and there was a good price on these. Working like a battery should. Will purchase again when these are used up\n",
      "\n",
      "Stage 1 Output:\n",
      "positive: Good price on batteries; Battery works as expected; Will purchase again when batteries are used up negative: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:   0%|          | 16/10000 [00:46<7:55:51,  2.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stage 2 Output:\n",
      "Positive: affordable batteries, satisfactory battery performance, intent to return after battery use. Negative: None.\n",
      "\n",
      "--- Processing Review 17 ---\n",
      "Input:\n",
      "The book is slightly worn as was mentioned in the description so no disappointment there. What surprised me was just how much my 8 1/2 yr old son enjoyed reading this book. He's a good reader and the story's length was just a little on the short side but the words offered him a fairly adequate challenge. Having great pictures to go with was an added bonus since he normally reads chapter books. It's the book's great imaginative story that seems to have won him over. He read it 3 times in 2 days and it still sits on his table ready to read again. The moral of the story is what won me over - showing that talents are very helpful, but it's kindness in leadership that pull things together and makes the greatest difference in life for you and everyone you meet. I give this book a 5+.\n",
      "\n",
      "Stage 1 Output:\n",
      "positive: The book is enjoyable and well-written; The book's imaginative story has won the child over; The book is ready to read again after reading; The book's moral is a positive example of kindness in leadership; The book is rated 5+ negative: The book is slightly worn as described in the description; The story's length was just a little on the short side\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:   0%|          | 17/10000 [00:50<8:08:18,  2.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stage 2 Output:\n",
      ": enjoyable and well-written book, imaginative story appeal, readability, positive moral example, high rating. Cons: worn condition, short story length.\n",
      "\n",
      "--- Processing Review 18 ---\n",
      "Input:\n",
      "This woman does not seem to care about truth. She twists quotes and facts around to prove her point, often when they actually prove the exact opposite. She makes the right look like nothing but moronic, hypocritical, unintelligent liars. The assertions she makes are completely off the wall, and her \"evidence\" is never evident about anything. In the past, Coulter has advocated mass murder of innocent people, essentialism of the worst kind, and has criticized wounder Vietnam vets for their \"treasonous remarks\" when all they were doing was pointing out some mistakes the army has made and how they can learn from them in order to make America safer. Thanks Ann, for making the American political debate a little bit dumber.\n",
      "\n",
      "Stage 1 Output:\n",
      "positive: None negative: Coulter does not care about truth; She twists quotes and facts around to prove her point; She makes the right look like moronic, hypocritical, and unintelligent liars; Her assertions are completely off the wall; Evidence is never evident about anything; Coulter has advocated mass murder of innocent people; Coulter has criticized wounder Vietnam vets for their \"treasonous remarks\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:   0%|          | 18/10000 [00:54<9:06:40,  3.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stage 2 Output:\n",
      ": None negative: lack of truth-oriented behavior, manipulative quotes and facts, misleading statements about the right, unsubstantiated assertions, lack of evidence, support for mass murder, criticism of Vietnam vets for \"treasonous remarks\"\n",
      "\n",
      "--- Processing Review 19 ---\n",
      "Input:\n",
      "This is one of the most [boring book] I ever tried to read. I couldn't take any more than the first 25 pages. I would like to set it on fire...that would be about the only thing that would make this nasty little piece of work interesting. I regret spending any money on this book. It's not good enough to give away...they should pay people to take it.\n",
      "\n",
      "Stage 1 Output:\n",
      "positive: None negative: The book is very boring and difficult to read.; The first 25 pages are too long for the reviewer.; The reviewer regrets spending money on the book.; The book is not good enough to give away.; The reviewer wants to set the book on fire.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:   0%|          | 19/10000 [00:56<8:26:41,  3.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stage 2 Output:\n",
      "Pros: None negative: boring and difficult to read, excessively long first pages, regret over spending, inadequate value for giveaways, desire to set the book on fire.\n",
      "\n",
      "--- Processing Review 20 ---\n",
      "Input:\n",
      "I have been an E. Lynn Harris fan for years! This latest book was just not interesting to me. It seemed like it was the same old plot, nothing really new or exciting. Normally, I read his books within a couple of days. This one was kind of boring to me. Maybe I was expecting too much, but this one didn't do it for me. The characters lacked depth and the plot wasn't juicy like in his last books. I'd hate to stop being a fan and hopefully I'm not \"outgrowing\" his books. PLEASE produce something better next time, Mr. Harris!\n",
      "\n",
      "Stage 1 Output:\n",
      "positive: E. Lynn Harris fans for years; Normally read his books within a couple of days negative: Newer book was boring; Characters lacked depth; Plot wasn't juicy like in previous books; Overall experience was boring\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Predicting:   0%|          | 19/10000 [00:58<8:30:26,  3.07s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 130\u001b[0m\n\u001b[1;32m    125\u001b[0m                 correct_predictions \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    126\u001b[0m             \u001b[38;5;66;03m# You can uncomment the line below for a more detailed look at the comparison.\u001b[39;00m\n\u001b[1;32m    127\u001b[0m             \u001b[38;5;66;03m# print(f\"{sentence} | T5: {label} | BERT: {bert_label}\")\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \n\u001b[1;32m    129\u001b[0m     \u001b[38;5;66;03m# Stage 2: Now I'll summarize the pros and cons that were identified in Stage 1.\u001b[39;00m\n\u001b[0;32m--> 130\u001b[0m     stage2_output \u001b[38;5;241m=\u001b[39m \u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstage1_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mStage 2 Output:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mstage2_output\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    133\u001b[0m \u001b[38;5;66;03m# --- Final Results ---\u001b[39;00m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;66;03m# I'll calculate and print the final accuracy score.\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[1], line 55\u001b[0m, in \u001b[0;36mpredict\u001b[0;34m(text, model, tokenizer, max_length)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;124;03mI wrote this simple helper function to get predictions from a T5 model.\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;124;03mIt takes text, tokenizes it, and returns the generated output.\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     54\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(text, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 55\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mdecode(outputs[\u001b[38;5;241m0\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/generation/utils.py:2623\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[1;32m   2615\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2616\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2617\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2618\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2619\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2620\u001b[0m     )\n\u001b[1;32m   2622\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2623\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2624\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2625\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2626\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2627\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2628\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2629\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2630\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2631\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2633\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2634\u001b[0m     \u001b[38;5;66;03m# 11. interleave input_ids with `num_beams` additional sequences per batch\u001b[39;00m\n\u001b[1;32m   2635\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2636\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2637\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[1;32m   2638\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2639\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2640\u001b[0m     )\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/generation/utils.py:3604\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3601\u001b[0m model_inputs\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_hidden_states\u001b[39m\u001b[38;5;124m\"\u001b[39m: output_hidden_states} \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;28;01melse\u001b[39;00m {})\n\u001b[1;32m   3603\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_prefill:\n\u001b[0;32m-> 3604\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   3605\u001b[0m     is_prefill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   3606\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/models/t5/modeling_t5.py:1792\u001b[0m, in \u001b[0;36mT5ForConditionalGeneration.forward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1789\u001b[0m         decoder_attention_mask \u001b[38;5;241m=\u001b[39m decoder_attention_mask\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder\u001b[38;5;241m.\u001b[39mfirst_device)\n\u001b[1;32m   1791\u001b[0m \u001b[38;5;66;03m# Decode\u001b[39;00m\n\u001b[0;32m-> 1792\u001b[0m decoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1793\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1795\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1801\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1802\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1803\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1804\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1805\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1806\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1808\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m decoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1810\u001b[0m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/models/t5/modeling_t5.py:1105\u001b[0m, in \u001b[0;36mT5Stack.forward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states:\n\u001b[1;32m   1103\u001b[0m     all_hidden_states \u001b[38;5;241m=\u001b[39m all_hidden_states \u001b[38;5;241m+\u001b[39m (hidden_states,)\n\u001b[0;32m-> 1105\u001b[0m layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1106\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1107\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1108\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1109\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1110\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# as a positional argument for gradient checkpointing\u001b[39;49;00m\n\u001b[1;32m   1112\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1113\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1114\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1115\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1116\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1117\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1118\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1119\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1121\u001b[0m \u001b[38;5;66;03m# layer_outputs is a tuple with:\u001b[39;00m\n\u001b[1;32m   1122\u001b[0m \u001b[38;5;66;03m# hidden-states, key-value-states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)\u001b[39;00m\n\u001b[1;32m   1123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/modeling_layers.py:83\u001b[0m, in \u001b[0;36mGradientCheckpointingLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     80\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(message)\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), \u001b[38;5;241m*\u001b[39margs)\n\u001b[0;32m---> 83\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/models/t5/modeling_t5.py:702\u001b[0m, in \u001b[0;36mT5Block.forward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    700\u001b[0m do_cross_attention \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_decoder \u001b[38;5;129;01mand\u001b[39;00m encoder_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    701\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_cross_attention:\n\u001b[0;32m--> 702\u001b[0m     cross_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    703\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    704\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_value_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    706\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    707\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    708\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    710\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    711\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    712\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    713\u001b[0m     hidden_states, past_key_value \u001b[38;5;241m=\u001b[39m cross_attention_outputs[:\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m    715\u001b[0m     \u001b[38;5;66;03m# clamp inf values to enable fp16 training\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/models/t5/modeling_t5.py:631\u001b[0m, in \u001b[0;36mT5LayerCrossAttention.forward\u001b[0;34m(self, hidden_states, key_value_states, attention_mask, position_bias, layer_head_mask, past_key_value, use_cache, query_length, output_attentions, cache_position)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    619\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    620\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    629\u001b[0m     cache_position\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    630\u001b[0m ):\n\u001b[0;32m--> 631\u001b[0m     normed_hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mEncDecAttention(\n\u001b[1;32m    633\u001b[0m         normed_hidden_states,\n\u001b[1;32m    634\u001b[0m         mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    642\u001b[0m         cache_position\u001b[38;5;241m=\u001b[39mcache_position,\n\u001b[1;32m    643\u001b[0m     )\n\u001b[1;32m    644\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m hidden_states \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(attention_output[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/models/t5/modeling_t5.py:254\u001b[0m, in \u001b[0;36mT5LayerNorm.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states):\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;66;03m# T5 uses a layer_norm which only scales and doesn't shift, which is also known as Root Mean\u001b[39;00m\n\u001b[1;32m    250\u001b[0m     \u001b[38;5;66;03m# Square Layer Normalization https://huggingface.co/papers/1910.07467 thus variance is calculated\u001b[39;00m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;66;03m# w/o mean and there is no bias. Additionally we want to make sure that the accumulation for\u001b[39;00m\n\u001b[1;32m    252\u001b[0m     \u001b[38;5;66;03m# half-precision inputs is done in fp32\u001b[39;00m\n\u001b[0;32m--> 254\u001b[0m     variance \u001b[38;5;241m=\u001b[39m \u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    255\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m hidden_states \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mrsqrt(variance \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvariance_epsilon)\n\u001b[1;32m    257\u001b[0m     \u001b[38;5;66;03m# convert into half-precision if necessary\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#loading the tokenizers and models for each stage.\n",
    "#use MPS if it's available for faster processing on my Mac, otherwise I'll use the CPU.\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "\n",
    "tokenizer1 = T5Tokenizer.from_pretrained(stage1_path)\n",
    "model1 = T5ForConditionalGeneration.from_pretrained(stage1_path).to(device)\n",
    "\n",
    "tokenizer2 = T5Tokenizer.from_pretrained(stage2_path)\n",
    "model2 = T5ForConditionalGeneration.from_pretrained(stage2_path).to(device)\n",
    "\n",
    "\n",
    "def predict(text, model, tokenizer, max_length=512):\n",
    "    \"\"\"\n",
    "    I wrote this simple helper function to get predictions from a T5 model.\n",
    "    It takes text, tokenizes it, and returns the generated output.\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "    outputs = model.generate(inputs[\"input_ids\"], max_length=max_length)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "def parse_output(output_str):\n",
    "    \"\"\"\n",
    "    I created this function to parse the model's output string into a dictionary.\n",
    "    The format is expected to be 'positive: ...; negative: ...'\n",
    "    \"\"\"\n",
    "    # I'll use a regex to capture the text for the 'positive' and 'negative' sections.\n",
    "    match = re.search(r\"positive:\\s*(.*?)\\s*negative:\\s*(.*)\", output_str, re.IGNORECASE | re.DOTALL)\n",
    "    if not match:\n",
    "        return {\"positive\": [], \"negative\": []}\n",
    "\n",
    "    pos_text, neg_text = match.groups()\n",
    "\n",
    "    # I'll split the text by semicolons and clean up any extra whitespace.\n",
    "    positives = [p.strip() for p in pos_text.split(\";\") if p.strip()]\n",
    "    negatives = [n.strip() for n in neg_text.split(\";\") if n.strip()]\n",
    "\n",
    "    return {\"positive\": positives, \"negative\": negatives}\n",
    "\n",
    "\n",
    "# --- Setting up the Evaluation Tool (DistilBERT) ---\n",
    "# Using a pre-trained sentiment analysis model (DistilBERT) to evaluate\n",
    "\n",
    "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "bert_model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "bert_model.to(device)\n",
    "\n",
    "\n",
    "def classify_sentiment(sentence: str) -> str:\n",
    "    \"\"\"\n",
    "    I built this function to classify a sentence as 'positive' or 'negative' using DistilBERT.\n",
    "    \"\"\"\n",
    "    inputs = bert_tokenizer(sentence, return_tensors=\"pt\", truncation=True, padding=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = bert_model(**inputs)\n",
    "    prediction_id = torch.argmax(outputs.logits, dim=-1).item()\n",
    "    return [\"negative\", \"positive\"][prediction_id]\n",
    "\n",
    "\n",
    "# --- Main Inference & Evaluation Loop ---\n",
    "# Run my reviews through the two T5 models and see what the results are.\n",
    "# Use DistilBERT to calculate an accuracy score.\n",
    "\n",
    "total_sentences = 0\n",
    "correct_predictions = 0\n",
    "\n",
    "for idx, review in enumerate(tqdm(input_reviews, desc=\"Predicting\")):\n",
    "    print(f\"\\n--- Processing Review {idx + 1} ---\")\n",
    "    print(f\"Input:\\n{review}\")\n",
    "\n",
    "    # Stage 1: get the sentence classifications (pros and cons).\n",
    "    stage1_output = predict(review, model1, tokenizer1)\n",
    "    \n",
    "    # parse the output into a dictionary to make it easy to work with.\n",
    "    parsed = parse_output(stage1_output)\n",
    "    \n",
    "    print(f\"\\nStage 1 Output:\\n{stage1_output}\")\n",
    "    \n",
    "    # check the accuracy of the Stage 1 output against DistilBERT.\n",
    "    # iterate through the classified sentences and compare them.\n",
    "    for label, sentences in parsed.items():\n",
    "        for sentence in sentences:\n",
    "            total_sentences += 1\n",
    "            bert_label = classify_sentiment(sentence)\n",
    "            if bert_label == label:\n",
    "                correct_predictions += 1\n",
    "            # You can uncomment the line below for a more detailed look at the comparison.\n",
    "            # print(f\"{sentence} | T5: {label} | BERT: {bert_label}\")\n",
    "\n",
    "    # Stage 2: summarize the pros and cons that were identified in Stage 1.\n",
    "    stage2_output = predict(stage1_output, model2, tokenizer2)\n",
    "    print(f\"\\nStage 2 Output:\\n{stage2_output}\")\n",
    "\n",
    "# --- Final Results ---\n",
    "# calculate and print the final accuracy score.\n",
    "accuracy = correct_predictions / total_sentences if total_sentences > 0 else 0\n",
    "print(f\"\\nAccuracy of Stage 1 model against DistilBERT = {accuracy:.2%}\")\n",
    "\n",
    "# save the accuracy score to a file.\n",
    "file_name = \"Stage1_accuracy.txt\"\n",
    "with open(file_name, 'w') as file:\n",
    "    # I need to convert the float to a string before I can write it to the file.\n",
    "    file.write(str(accuracy))\n",
    "\n",
    "print(f\"Accuracy value successfully saved to '{file_name}'.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
